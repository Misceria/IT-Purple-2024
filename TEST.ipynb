{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9139cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debea463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Полезные частые функции\n",
    "\n",
    "def metrics(y_true, y_preds):\n",
    "    acc = sklearn.metrics.accuracy_score(y_true, y_preds)  \n",
    "    rec = sklearn.metrics.recall_score(y_true, y_preds, average=None)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_preds)\n",
    "    prec = \tsklearn.metrics.precision_score(y_true, y_preds)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_true, y_preds)\n",
    "    print('test-accuracy:', acc, ' \\ntest-recall:', rec, '\\ntest-f1_score:', f1, '\\ntest-precision_score:', prec, '\\ntest-roc_auc_score:', auc)\n",
    "    \n",
    "\n",
    "\n",
    "def equallySample(data):\n",
    "    y_data = data[\"target\"]\n",
    "    x_data = data.drop(useless_columns, axis=1)\n",
    "    sample_size = 17000 # Задайте желаемое количество строк для каждого класса\n",
    "    frames = []\n",
    "    classes = data['target'].unique()\n",
    "\n",
    "    for i in classes:\n",
    "        if i == 0:  \n",
    "            g = data[data['target'] == i].sample(sample_size)\n",
    "        else:\n",
    "            g = data[data['target'] == i].sample(sample_size)\n",
    "        frames.append(g)\n",
    "\n",
    "    equally_sampled = pd.concat(frames)\n",
    "    equally_sampled['target'].value_counts()\n",
    "\n",
    "    X_train_equally_sampled, X_test_equally_sampled, y_train_equally_sampled,  y_test_equally_sampled = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "useless_columns = ['id', 'target', 'sample_ml_new', 'feature68', 'feature69', 'feature144', 'feature160',\n",
    "                   'feature292', 'feature406', 'feature407', 'feature496', 'feature511',\n",
    "                   'feature625', 'feature661', 'feature663', 'feature669', 'feature678',\n",
    "                   'feature683', 'feature686', 'feature710', 'feature756', 'feature761',\n",
    "                   'feature765', 'feature801', 'feature802', 'feature806', 'feature807',\n",
    "                   'feature808', 'feature809', 'feature816', 'feature818', 'feature819',\n",
    "                   'feature955', 'feature956', 'feature957', 'feature958', 'feature959',\n",
    "                   'feature960', 'feature961', 'feature962', 'feature963', 'feature964',\n",
    "                   'feature965', 'feature966', 'feature967', 'feature968', 'feature969',\n",
    "                   'feature970', 'feature971', 'feature972', 'feature973', 'feature974', \n",
    "                   'feature975', 'feature976', 'feature977', 'feature978', 'feature979',\n",
    "                   'feature980', 'feature981', 'feature982', 'feature983', 'feature984', \n",
    "                   'feature1005', 'feature1006', 'feature1007', 'feature1008', 'feature1009',\n",
    "                   'feature1010', 'feature1011', 'feature1012', 'feature1013', 'feature1014',\n",
    "                   'feature1015', 'feature1016', 'feature1017', 'feature1018', 'feature1019',\n",
    "                   'feature1020', 'feature1021', 'feature1022', 'feature1023', 'feature1024',\n",
    "                   'feature1025', 'feature1026', 'feature1027', 'feature1028', 'feature1029',\n",
    "                   'feature1030', 'feature1031', 'feature1032', 'feature1033', 'feature1034']\n",
    "\n",
    "\n",
    "y_data = data[\"target\"]\n",
    "x_data = data.drop(useless_columns, axis=1)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQual size of targets\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n",
    "\n",
    "\n",
    "sample_size = 17000 # Задайте желаемое количество строк для каждого класса\n",
    "frames = []\n",
    "classes = data['target'].unique()\n",
    "\n",
    "for i in classes:\n",
    "    if i == 0:  \n",
    "        g = data[data['target'] == i].sample(sample_size)\n",
    "    else:\n",
    "        g = data[data['target'] == i].sample(sample_size)\n",
    "    frames.append(g)\n",
    "\n",
    "equally_sampled = pd.concat(frames)\n",
    "equally_sampled['target'].value_counts()\n",
    "\n",
    "X_train_equally_sampled, X_test_equally_sampled, y_train_equally_sampled,  y_test_equally_sampled = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\"\"\"X_train_xgb_equally_sampled = xgb.DMatrix(X_train_equally_sampled, label=y_train_equally_sampled)\n",
    "X_test_xgb_equally_sampled = xgb.DMatrix(X_test_equally_sampled, label=y_test_equally_sampled)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "equally_sampled['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data = xgb.DMatrix(x_data, label=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87997bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 4,\n",
    "          'eta': 1,\n",
    "          'objective': \"binary:logistic\",\n",
    "          'nthread': -1,\n",
    "          'eval_metric': ['auc', 'map']}\n",
    "eval_list = [(X_test_xgb, \"Everything\")]\n",
    "num_rounds = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = dict()\n",
    "\n",
    "def objective(n_estimators, max_depth, classes_weight_o, learning_rate, gamma):\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    classes_weight_o = int(classes_weight_o)\n",
    "    classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight={0:1, 1:classes_weight_o},\n",
    "    y=y_train\n",
    ")   \n",
    "\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(  n_estimators=n_estimators,\n",
    "                                    max_depth=max_depth,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    booster=\"gbtree\",\n",
    "                                    n_jobs=-1,\n",
    "                                    eval_metric=['auc', 'pre'],\n",
    "                                    gamma=gamma)\n",
    "\n",
    "    xgb_model.fit(X_train, y_train, eval_set=eval_list, sample_weight=classes_weights)\n",
    "    preds = xgb_model.predict(X_test)\n",
    "    return sklearn.metrics.f1_score(y_test, preds)\n",
    "\n",
    "\"\"\"parameters = {\n",
    "    'max_depth': [4, 20],\n",
    "    'learning_rate': [0.2],\n",
    "    'n_estimators': [20, 40],\n",
    "}\n",
    "\n",
    "params = {'max_depth': 6,\n",
    "          'eta': 1,\n",
    "          'objective': \"binary:logistic\",\n",
    "          'nthread': -1,\n",
    "          'eval_metric': ['auc', 'pre'],\n",
    "          'class_weight': {0:0.1, 1:100}}\"\"\"\n",
    "eval_list = [(X_test, y_test)]\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\"\"\"classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight={0:1, 1:22},\n",
    "    y=y_train\n",
    ")\"\"\"\n",
    "#clf = sklearn.model_selection.GridSearchCV(xgb_model, parameters, scoring='f1_micro', n_jobs=-1, verbose=10)\n",
    "\n",
    "search_space = {\n",
    "    'max_depth': (4, 20),\n",
    "    'n_estimators': (10, 70),\n",
    "    \"classes_weight_o\": (10, 40),\n",
    "    'learning_rate': (0.001, 0.2),\n",
    "    'gamma': (0.01, 5)\n",
    "}\n",
    "import bayes_opt\n",
    "optimizer = bayes_opt.BayesianOptimization(\n",
    "                                f=objective,\n",
    "                                pbounds=search_space,\n",
    "                                random_state=12,\n",
    "                                allow_duplicate_points=True\n",
    "                              ) \n",
    "\n",
    "opt = optimizer.maximize(init_points=2, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight={0:1, 1:31},\n",
    "    y=y_train\n",
    ")   \n",
    "\n",
    "xgb_model = xgb.XGBClassifier(  gamma=4.8,\n",
    "                                learning_rate=0.152,\n",
    "                                max_depth=8,\n",
    "                                n_estimators=18,\n",
    "                                booster=\"gbtree\",\n",
    "                                n_jobs=-1,\n",
    "                                eval_metric=['auc', 'pre'],\n",
    "                                early_stopping_rounds=10)\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=eval_list, sample_weight=classes_weights)\n",
    "preds = xgb_model.predict(X_test)\n",
    "metrics(y_test, preds)\n",
    "\n",
    "\n",
    "\"\"\"{'classes_weight_o': 31.438781865708886,\n",
    " 'gamma': 4.806812881469824,\n",
    " 'learning_rate': 0.15290427033360873,\n",
    " 'max_depth': 8.582152510429442,\n",
    " 'n_estimators': 18.513852056962605}\n",
    "                                    test-accuracy: 0.7220024017736174  \n",
    "                                    test-recall: [0.72583707 0.61577552] \n",
    "                                    test-f1_score: 0.13371074915441264 \n",
    "                                    test-precision_score: 0.07499798175506579 \n",
    "                                    test-roc_auc_score: 0.6708062939632549\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_preds = xgb_model.predict(X_test)\n",
    "y_true = list(y_test)\n",
    "print(*y_preds)\n",
    "\n",
    "#ev = clf.eval(X_test)\n",
    "metrics(y_true, y_preds)\n",
    "#print(ev, ' eval-recall:')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_preds, y_test))\n",
    "cm = confusion_matrix(y_preds, y_test)\n",
    "acc = cm.diagonal().sum()/cm.sum()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4529c1",
   "metadata": {},
   "source": [
    "# GRIDSEARCH XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': range(2,10,2),\n",
    "    'learning_rate': np.linspace(.1, .6, 6),\n",
    "    'min_child_weight': range(1,10,2),\n",
    "}\n",
    "\n",
    "grid = sklearn.model_selection.GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier(n_jobs=-1,\n",
    "                              n_estimators=500,\n",
    "                              random_state=0),\n",
    "    param_grid = params,\n",
    "    scoring='f1',  # <------Use `scoring` instead of `eval_metric`\n",
    ")\n",
    "\n",
    "eval_set = [(X_train_equally_sampled, y_train_equally_sampled),\n",
    "            (X_test_equally_sampled, y_test_equally_sampled)]\n",
    "\n",
    "grid.fit(X_train_equally_sampled, y_train_equally_sampled,\n",
    "         eval_set=eval_set,\n",
    "         early_stopping_rounds=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af0bf2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['feature5'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea000a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['feature2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=140\n",
    "corr = data[['target', 'feature'+str(x), 'feature'+str(x+1), 'feature'+str(x+2), 'feature'+str(x+3), \n",
    "                'feature'+str(x+4), 'feature'+str(x+5), 'feature'+str(x+6), 'feature'+str(x+7), 'feature'+str(x+8),\n",
    "                'feature'+str(x+9)]].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ff513",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = data.boxplot(\"feature5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "\n",
    "useless_columns = ['id', 'target', 'sample_ml_new', 'feature68', 'feature69', 'feature144', 'feature160',\n",
    "                   'feature292', 'feature406', 'feature407', 'feature496', 'feature511',\n",
    "                   'feature625', 'feature661', 'feature663', 'feature669', 'feature678',\n",
    "                   'feature683', 'feature686', 'feature710', 'feature756', 'feature761',\n",
    "                   'feature765', 'feature801', 'feature802', 'feature806', 'feature807',\n",
    "                   'feature808', 'feature809', 'feature816', 'feature818', 'feature819',\n",
    "                   'feature955', 'feature956', 'feature957', 'feature958', 'feature959',\n",
    "                   'feature960', 'feature961', 'feature962', 'feature963', 'feature964',\n",
    "                   'feature965', 'feature966', 'feature967', 'feature968', 'feature969',\n",
    "                   'feature970', 'feature971', 'feature972', 'feature973', 'feature974', \n",
    "                   'feature975', 'feature976', 'feature977', 'feature978', 'feature979',\n",
    "                   'feature980', 'feature981', 'feature982', 'feature983', 'feature984', \n",
    "                   'feature1005', 'feature1006', 'feature1007', 'feature1008', 'feature1009',\n",
    "                   'feature1010', 'feature1011', 'feature1012', 'feature1013', 'feature1014',\n",
    "                   'feature1015', 'feature1016', 'feature1017', 'feature1018', 'feature1019',\n",
    "                   'feature1020', 'feature1021', 'feature1022', 'feature1023', 'feature1024',\n",
    "                   'feature1025', 'feature1026', 'feature1027', 'feature1028', 'feature1029',\n",
    "                   'feature1030', 'feature1031', 'feature1032', 'feature1033', 'feature1034']\n",
    "\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "y_data = data[\"target\"]\n",
    "\n",
    "x_data = data.drop(useless_columns, axis=1)\n",
    "print(x_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e45782",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\n",
    "X_train_xgb = xgb.DMatrix(X_train, label=y_train)\n",
    "X_test_xgb = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf370b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# impute\n",
    "train_data_num = X_train.select_dtypes(exclude=['object'])\n",
    "test_data_num = X_test.select_dtypes(exclude=['object'])\n",
    "imputer = SimpleImputer()\n",
    "train_num_cleaned = imputer.fit_transform(train_data_num)\n",
    "test_num_cleaned = imputer.transform(test_data_num)\n",
    "\n",
    "# columns rename after imputing\n",
    "train_num_cleaned = pd.DataFrame(train_num_cleaned)\n",
    "test_num_cleaned = pd.DataFrame(test_num_cleaned)\n",
    "train_num_cleaned.columns = train_data_num.columns\n",
    "test_num_cleaned.columns = test_data_num.columns\n",
    "\n",
    "\n",
    "cols_with_missing = [col for col in train_num_cleaned.columns\n",
    "                                if test_num_cleaned[col].isnull().any()]\n",
    "print(cols_with_missing)\n",
    "for col in cols_with_missing:\n",
    "    print(col, test_num_cleaned[col].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79906be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Изменение веса классов\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_data), y=y_data)\n",
    "\n",
    "# Создание модели с взвешиванием классов\n",
    "model = RandomForestClassifier(n_estimators=30, class_weight={0: 0.5, 1: 500.0})\n",
    "model.fit(train_num_cleaned, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_num_cleaned)\n",
    "preds = list(map(round, preds))\n",
    "y_true = list(y_test)\n",
    "print(preds.count(1))\n",
    "#ev = bst.eval(X_test_xgb)\n",
    "\n",
    "\n",
    "rec = sklearn.metrics.recall_score(y_true, preds)\n",
    "f1 = sklearn.metrics.f1_score(y_true, preds)\n",
    "prec = \tsklearn.metrics.precision_score(y_true, preds)\n",
    "auc = sklearn.metrics.roc_auc_score(y_true, preds)\n",
    "print(' test-recall:', rec, ' test-f1_score:', f1, ' test-precision_score:', prec, ' test-roc_auc_score:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cce962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Методы борьбы с дисбалансом данных\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "y_data = data[\"target\"]\n",
    "x_data = data.drop(useless_columns, axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\n",
    "# impute\n",
    "train_data_num = X_train.select_dtypes(exclude=['object'])\n",
    "test_data_num = X_test.select_dtypes(exclude=['object'])\n",
    "imputer = SimpleImputer()\n",
    "train_num_cleaned = imputer.fit_transform(train_data_num)\n",
    "test_num_cleaned = imputer.transform(test_data_num)\n",
    "\n",
    "# columns rename after imputing\n",
    "train_num_cleaned = pd.DataFrame(train_num_cleaned)\n",
    "test_num_cleaned = pd.DataFrame(test_num_cleaned)\n",
    "train_num_cleaned.columns = train_data_num.columns\n",
    "test_num_cleaned.columns = test_data_num.columns\n",
    "\n",
    "# Создание экземпляра SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Применение SMOTE к данным\n",
    "X_resampled, y_resampled = smote.fit_resample(train_num_cleaned, y_train)\n",
    "\n",
    "X_resampled_xgb = xgb.DMatrix(X_resampled, y_resampled)\n",
    "X_test_xgb = xgb.DMatrix(test_num_cleaned, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 8,\n",
    "          'eta': 1,\n",
    "          'objective': \"binary:logistic\",\n",
    "          'nthread': -1,\n",
    "          'eval_metric': ['auc', 'pre']}\n",
    "eval_list = [(X_test_xgb, \"Everything\")]\n",
    "num_rounds = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f23f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = dict()\n",
    "bst = xgb.train(params, X_resampled_xgb, num_rounds, eval_list, evals=X_test_xgb, evals_result=evals_result)\n",
    "y_preds = bst.predict(X_test_xgb)\n",
    "preds = list(map(round, preds))\n",
    "y_true = list(y_test)\n",
    "metrics(y_true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad67914",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение каждой моделей члена на подмножестве данных\n",
    "\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "y_data = data[\"target\"]\n",
    "x_data = data.drop(useless_columns, axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\n",
    "# impute\n",
    "train_data_num = X_train.select_dtypes(exclude=['object'])\n",
    "test_data_num = X_test.select_dtypes(exclude=['object'])\n",
    "imputer = SimpleImputer()\n",
    "train_num_cleaned = imputer.fit_transform(train_data_num)\n",
    "test_num_cleaned = imputer.transform(test_data_num)\n",
    "\n",
    "# columns rename after imputing\n",
    "train_num_cleaned = pd.DataFrame(train_num_cleaned)\n",
    "test_num_cleaned = pd.DataFrame(test_num_cleaned)\n",
    "train_num_cleaned.columns = train_data_num.columns\n",
    "test_num_cleaned.columns = test_data_num.columns\n",
    "\n",
    "X_train_xgb = xgb.DMatrix(train_num_cleaned, y_train)\n",
    "X_test_xgb = xgb.DMatrix(test_num_cleaned, y_train)\n",
    "\n",
    "eval_list = [(X_test_xgb, \"Everything\")]\n",
    "num_rounds = 40\n",
    "ensemble = [xgb.train({'max_depth': x,\n",
    "          'eta': 1,\n",
    "          'objective': \"binary:logistic\",\n",
    "          'nthread': -1,\n",
    "          'eval_metric': ['auc', 'pre']}, X_train_xgb, num_rounds, eval_list, evals=X_test_xgb, evals_result=evals_result) for x in range(2, 10)]\n",
    "ind = 1\n",
    "\n",
    "\"\"\"for model in ensemble:\n",
    "    sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_subset, y_subset = X_train[sample_indices], y_train[sample_indices]\n",
    "    model.fit(X_subset, y_subset)\n",
    "    print(f\"Модель {ind} обучена\")\n",
    "    ind +=1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "predictions = np.array([model.predict(X_test_xgb) for model in ensemble])\n",
    "new_predictions=[]\n",
    "for x in range(len(predictions[0])):\n",
    "    new_predictions.append(np.array([predictions[y][x] for y in range(len(predictions))]))\n",
    "ensemble_predictions = [int(mode(x.round())) for x in new_predictions]\n",
    "# Используем np.argmax для нахождения индекса наиболее часто встречающегося значения\n",
    "\n",
    "metrics(y_test, ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb26519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кросс-обучение\n",
    "\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "y_data = data[\"target\"]\n",
    "x_data = data.drop(useless_columns, axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\n",
    "# impute\n",
    "train_data_num = X_train.select_dtypes(exclude=['object'])\n",
    "test_data_num = X_test.select_dtypes(exclude=['object'])\n",
    "imputer = SimpleImputer()\n",
    "train_num_cleaned = imputer.fit_transform(train_data_num)\n",
    "test_num_cleaned = imputer.transform(test_data_num)\n",
    "\n",
    "# columns rename after imputing\n",
    "train_num_cleaned = pd.DataFrame(train_num_cleaned)\n",
    "test_num_cleaned = pd.DataFrame(test_num_cleaned)\n",
    "train_num_cleaned.columns = train_data_num.columns\n",
    "test_num_cleaned.columns = test_data_num.columns\n",
    "\n",
    "X_train_xgb = xgb.DMatrix(train_num_cleaned, y_train)\n",
    "X_test_xgb = xgb.DMatrix(test_num_cleaned, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "y_data = data[\"target\"]\n",
    "x_data = data.drop(useless_columns, axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\n",
    "# impute\n",
    "train_data_num = X_train.select_dtypes(exclude=['object'])\n",
    "test_data_num = X_test.select_dtypes(exclude=['object'])\n",
    "imputer = SimpleImputer()\n",
    "train_num_cleaned = imputer.fit_transform(train_data_num)\n",
    "test_num_cleaned = imputer.transform(test_data_num)\n",
    "\n",
    "# columns rename after imputing\n",
    "train_num_cleaned = pd.DataFrame(train_num_cleaned)\n",
    "test_num_cleaned = pd.DataFrame(test_num_cleaned)\n",
    "train_num_cleaned.columns = train_data_num.columns\n",
    "test_num_cleaned.columns = test_data_num.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449b0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucking useless parts of code\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "\n",
    "columns_with_few_unique_values = ['feature642', 'feature756', 'sample_ml_new', 'id']\n",
    "\n",
    "drop_colums = {'feature642', 'feature756', 'sample_ml_new', 'id'}\n",
    "for col in data.iloc[:, 3:]:\n",
    "    if np.unique(data[col]).shape == (1,):\n",
    "        drop_colums.add(col)\n",
    "correl = data.drop(['sample_ml_new', 'id'], axis=1).corr()\n",
    "for i in correl.iloc[1:, 1][np.asarray(np.abs(correl.iloc[1:, 1]) < 0.0003)].index:\n",
    "    drop_colums.add(col)\n",
    "    columns_with_few_unique_values.append(col)\n",
    "#print(*columns_with_few_unique_values, sep=\"\\n\")\n",
    "drop_colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d784234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0         11457\n",
      "1         11323\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "data = pd.read_parquet('train_data.parquet', engine='pyarrow')\n",
    "\n",
    "useless_columns = ['id', 'sample_ml_new', 'feature68', 'feature69', 'feature144', 'feature160',\n",
    "                   'feature292', 'feature406', 'feature407', 'feature496', 'feature511',\n",
    "                   'feature625', 'feature661', 'feature663', 'feature669', 'feature678',\n",
    "                   'feature683', 'feature686', 'feature710', 'feature756', 'feature761',\n",
    "                   'feature765', 'feature801', 'feature802', 'feature806', 'feature807',\n",
    "                   'feature808', 'feature809', 'feature816', 'feature818', 'feature819',\n",
    "                   'feature955', 'feature956', 'feature957', 'feature958', 'feature959',\n",
    "                   'feature960', 'feature961', 'feature962', 'feature963', 'feature964',\n",
    "                   'feature965', 'feature966', 'feature967', 'feature968', 'feature969',\n",
    "                   'feature970', 'feature971', 'feature972', 'feature973', 'feature974', \n",
    "                   'feature975', 'feature976', 'feature977', 'feature978', 'feature979',\n",
    "                   'feature980', 'feature981', 'feature982', 'feature983', 'feature984', \n",
    "                   'feature1005', 'feature1006', 'feature1007', 'feature1008', 'feature1009',\n",
    "                   'feature1010', 'feature1011', 'feature1012', 'feature1013', 'feature1014',\n",
    "                   'feature1015', 'feature1016', 'feature1017', 'feature1018', 'feature1019',\n",
    "                   'feature1020', 'feature1021', 'feature1022', 'feature1023', 'feature1024',\n",
    "                   'feature1025', 'feature1026', 'feature1027', 'feature1028', 'feature1029',\n",
    "                   'feature1030', 'feature1031', 'feature1032', 'feature1033', 'feature1034']\n",
    "#useless_columns = ['id', *columns_with_few_unique_values[1:]]\n",
    "\n",
    "y_data = data[\"target\"]\n",
    "x_data = data.drop([*useless_columns, 'target'], axis=1)\n",
    "\n",
    "sample_size = 17000 # Задайте желаемое количество строк для каждого класса\n",
    "frames = []\n",
    "classes = data['target'].unique()\n",
    "\n",
    "for i in classes:\n",
    "    if i == 0:  \n",
    "        g = data[data['target'] == i].sample(sample_size)\n",
    "    else:\n",
    "        g = data[data['target'] == i].sample(sample_size)\n",
    "    frames.append(g)\n",
    "\n",
    "equally_sampled = pd.concat(frames)\n",
    "equally_sampled = equally_sampled.drop(useless_columns, axis=1)\n",
    "y_equally_sampled = pd.DataFrame(equally_sampled['target'])\n",
    "x_equally_sampled = pd.DataFrame(equally_sampled.drop(['target'], axis=1))\n",
    "\n",
    "X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_equally_sampled, y_equally_sampled, test_size=0.33, shuffle=True)\n",
    "\n",
    "#X_train, X_test, y_train,  y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.25, shuffle=True)\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "# ЗАПОЛНЕНИЕ ПУСТОТ TRAIN и TEST\n",
    "train_data_num = X_train.select_dtypes(exclude=['object'])\n",
    "test_data_num = X_test.select_dtypes(exclude=['object'])\n",
    "imputer = SimpleImputer()\n",
    "train_num_cleaned = imputer.fit_transform(train_data_num)\n",
    "test_num_cleaned = imputer.transform(test_data_num)\n",
    "\n",
    "# columns rename after imputing\n",
    "train_num_cleaned = pd.DataFrame(train_num_cleaned)\n",
    "test_num_cleaned = pd.DataFrame(test_num_cleaned)\n",
    "train_num_cleaned.columns = train_data_num.columns\n",
    "test_num_cleaned.columns = test_data_num.columns\n",
    "print(y_train.value_counts())\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# УДАЛЕНИЕ ВЫБРОСОВ\n",
    "clf = IsolationForest(contamination=0.05)\n",
    "clf.fit(train_num_cleaned)\n",
    "y_pred_train = clf.predict(train_num_cleaned)\n",
    "y_pred_test = clf.predict(test_num_cleaned)\n",
    "\n",
    "train_num_cleaned = train_num_cleaned[y_pred_train == 1]\n",
    "y_train = y_train[y_pred_train == 1]\n",
    "test_num_cleaned = test_num_cleaned[y_pred_test == 1]\n",
    "y_test = y_test[y_pred_test == 1]\n",
    "\n",
    "# НОРМАЛИЗАЦИЯ\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_num_cleaned)\n",
    "data_train = scaler.transform(train_num_cleaned)\n",
    "data_test = scaler.transform(test_num_cleaned)\n",
    "\n",
    "\n",
    "\"\"\"# ВЫРАВНИВАНИЕ ДИСБАЛАНСА КЛАССОВ\n",
    "ada = ADASYN(sampling_strategy='auto', random_state=0, n_neighbors=5)\n",
    "data_train, y_train = ada.fit_resample(data_train, y_train)\n",
    "print(y_train.value_counts())\n",
    "\"\"\"\n",
    "\n",
    "# ПОДГОТОВКА ВСЕГО ДАТАСЕТА\n",
    "all_data = x_data.select_dtypes(exclude=['object'])\n",
    "all_data_cleaned = imputer.transform(all_data)\n",
    "y_pred = clf.predict(all_data_cleaned)\n",
    "all_data_cleaned = all_data_cleaned[y_pred == 1]\n",
    "y_data = y_data[y_pred==1]\n",
    "all_data_cleaned = pd.DataFrame(all_data_cleaned)\n",
    "all_data_cleaned.columns = x_data.columns\n",
    "all_data = scaler.transform(all_data_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a869f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78875639, 1.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.70300965, 1.        , 0.14184397, ..., 0.53973893, 0.54103455,\n",
       "        0.49632237],\n",
       "       ...,\n",
       "       [0.97444634, 0.97555429, 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.9290176 , 0.04604889, 0.88652482, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 0.14893617, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536eed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21641, 987)\n",
      "(21641, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "233b0d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_49\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_49\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_159 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">104,576</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_160 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_161 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_162 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_159 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m104,576\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_62 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_160 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_63 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_161 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m528\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_162 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,249</span> (426.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,249\u001b[0m (426.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,249</span> (426.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,249\u001b[0m (426.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, input_shape=(816, ), activation='relu'),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "class_weights = {0: 0.5,\n",
    "                 1: 1.1}\n",
    "model.compile(optimizer=\"sgd\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "329cd18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.5562 - loss: 0.6883 - val_AUC: 0.6530 - val_loss: 0.6839\n",
      "Epoch 2/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6322 - loss: 0.6652 - val_AUC: 0.6705 - val_loss: 0.6473\n",
      "Epoch 3/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6503 - loss: 0.6578 - val_AUC: 0.6748 - val_loss: 0.6990\n",
      "Epoch 4/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6611 - loss: 0.6526 - val_AUC: 0.6819 - val_loss: 0.6546\n",
      "Epoch 5/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6679 - loss: 0.6486 - val_AUC: 0.6838 - val_loss: 0.6613\n",
      "Epoch 6/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6812 - loss: 0.6416 - val_AUC: 0.6854 - val_loss: 0.7046\n",
      "Epoch 7/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6854 - loss: 0.6390 - val_AUC: 0.6874 - val_loss: 0.6486\n",
      "Epoch 8/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6946 - loss: 0.6331 - val_AUC: 0.6894 - val_loss: 0.6873\n",
      "Epoch 9/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7015 - loss: 0.6295 - val_AUC: 0.6886 - val_loss: 0.6589\n",
      "Epoch 10/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7064 - loss: 0.6258 - val_AUC: 0.6895 - val_loss: 0.6057\n",
      "Epoch 11/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7144 - loss: 0.6205 - val_AUC: 0.6880 - val_loss: 0.6493\n",
      "Epoch 12/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7171 - loss: 0.6189 - val_AUC: 0.6901 - val_loss: 0.6387\n",
      "Epoch 13/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7236 - loss: 0.6134 - val_AUC: 0.6867 - val_loss: 0.6216\n",
      "Epoch 14/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7285 - loss: 0.6100 - val_AUC: 0.6885 - val_loss: 0.5939\n",
      "Epoch 15/15\n",
      "\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7339 - loss: 0.6065 - val_AUC: 0.6916 - val_loss: 0.6294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x225cc1bd690>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=data_train, y=y_train, batch_size=128, epochs=15, verbose=1, validation_data=(data_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1c6f1737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15341/15341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 624us/step - AUC: 0.7036 - loss: 0.6108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6122022867202759, 0.7066034078598022]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(all_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d8eba100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">104,576</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m104,576\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,707</span> (409.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,707\u001b[0m (409.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,705</span> (409.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,705\u001b[0m (409.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15341/15341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 572us/step\n",
      "thr =  0.5\n",
      "test-accuracy: 0.6749534530313586  \n",
      "test-recall: [0.67548783 0.66052587] \n",
      "test-f1_score: 0.12675670942603215 \n",
      "test-precision_score: 0.07010502739187022 \n",
      "test-roc_auc_score: 0.6680068492349086\n",
      "thr =  0.4\n",
      "test-accuracy: 0.5046485856647559  \n",
      "test-recall: [0.49355577 0.80414076] \n",
      "test-f1_score: 0.10391093979739614 \n",
      "test-precision_score: 0.05554417453926582 \n",
      "test-roc_auc_score: 0.6488482652045351\n",
      "thr =  0.6\n",
      "test-accuracy: 0.802840485473679  \n",
      "test-recall: [0.8140119  0.50122626] \n",
      "test-f1_score: 0.15368741365138772 \n",
      "test-precision_score: 0.0907579340899937 \n",
      "test-roc_auc_score: 0.6576190804872106\n"
     ]
    }
   ],
   "source": [
    "proba = model.predict(all_data)\n",
    "threshold = 0.5 # Установите свой порог\n",
    "predictions = (proba > threshold).astype('int32')\n",
    "print(\"thr = \", threshold)\n",
    "metrics(y_data, predictions)\n",
    "threshold = 0.4 # Установите свой порог\n",
    "predictions = (proba > threshold).astype('int32')\n",
    "print(\"thr = \", threshold)\n",
    "metrics(y_data, predictions)\n",
    "threshold = 0.6 # Установите свой порог\n",
    "predictions = (proba > threshold).astype('int32')\n",
    "print(\"thr = \", threshold)\n",
    "metrics(y_data, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ca778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_ver_7330.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_ver_7330.h5\"\n",
    "\n",
    "model_to_subm = tf.keras.models.load_model(model_name)\n",
    "model_to_subm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO LOAD DATA AND SUBMISSION\n",
    "\n",
    "model_to_subm = tf.keras.models.load_model(model_name)\n",
    "#model_to_subm.evaluate(all_data, y_data)\n",
    "\n",
    "test_dataset = pd.read_parquet('test_sber.parquet', engine='pyarrow')\n",
    "useless_columns = ['id', *columns_with_few_unique_values[1:]]\n",
    "\n",
    "test_dataset = test_dataset.drop(useless_columns, axis=1)\n",
    "test_dataset_cleaned = imputer.transform(test_dataset)\n",
    "test_dataset_cleaned = pd.DataFrame(test_dataset_cleaned)\n",
    "test_dataset_cleaned.columns = test_dataset.columns\n",
    "test_dataset = scaler.transform(test_dataset_cleaned)\n",
    "\n",
    "pred = model_to_subm.predict(test_dataset)\n",
    "pred_binary = np.round(pred).astype(int)\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"target_prob\"] = pred\n",
    "submission[\"target_bin\"] = pred_binary\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ec748830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   42.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   42.7s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, verbose=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, verbose=2)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.linear_model.LogisticRegression(max_iter=1000, \n",
    "                                                verbose=2, \n",
    "                                                n_jobs=-1)\n",
    "\n",
    "model.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "93f807c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-accuracy: 0.6654883940143749  \n",
      "test-recall: [0.79762623 0.47866894] \n",
      "test-f1_score: 0.5424657534246575 \n",
      "test-precision_score: 0.6258832279657865 \n",
      "test-roc_auc_score: 0.6381475870629858\n"
     ]
    }
   ],
   "source": [
    "metrics(model.predict(data_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bb82ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d4d4b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-accuracy: 0.38824083893012845  \n",
      "test-recall: [0.74105012 0.3269018 ] \n",
      "test-f1_score: 0.47656013711059586 \n",
      "test-precision_score: 0.8789512830048345 \n",
      "test-roc_auc_score: 0.533975958697683\n"
     ]
    }
   ],
   "source": [
    "metrics(model.predict(data_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2df2bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "02b3db89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\AppData\\Local\\Temp\\ipykernel_17960\\1731829732.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(data_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9b0bca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-accuracy: 0.7248733356898787  \n",
      "test-recall: [0.75493227 0.60442478] \n",
      "test-f1_score: 0.46738138686131386 \n",
      "test-precision_score: 0.3809966530308665 \n",
      "test-roc_auc_score: 0.679678526011862\n"
     ]
    }
   ],
   "source": [
    "metrics(model.predict(data_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8ece6dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "model.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4c2b5b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-accuracy: 0.6931188877106162  \n",
      "test-recall: [0.75669158 0.51864108] \n",
      "test-f1_score: 0.474422358995056 \n",
      "test-precision_score: 0.43715135738192634 \n",
      "test-roc_auc_score: 0.6376663304137686\n"
     ]
    }
   ],
   "source": [
    "metrics(model.predict(data_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1bedcacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/4c/6a/219a431aaf81b3eb3070fd2d58116baa366d3072f43bbcc87dc3495b7546/optuna-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/7f/50/9fb3a5c80df6eb6516693270621676980acd6d5a9a7efdbfa273f8d616c7/alembic-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/f3/18/3e867ab37a24fdf073c1617b9c7830e06ec270b1ea4694a624038fc40a03/colorlog-6.8.2-py3-none-any.whl.metadata\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/2b/8d/9f11d0b9ac521febb806e7f30dc5982d0f4f5821217712c59005fbc5c1e3/Mako-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading Mako-1.3.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\kseni\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/413.4 kB 660.6 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/413.4 kB 393.8 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 92.2/413.4 kB 751.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 143.4/413.4 kB 774.0 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 153.6/413.4 kB 762.6 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 235.5/413.4 kB 846.9 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 317.4/413.4 kB 981.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  409.6/413.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 61.4/233.4 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 153.6/233.4 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 233.4/233.4 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.7/78.7 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.2 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b44816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-13 16:16:09,747] A new study created in memory with name: no-name-cf98d91a-d262-4bff-bcbb-f18afad2eebc\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:16:36,674] Trial 0 finished with value: 0.822375440008067 and parameters: {'n_estimators': 176, 'criterion': 'friedman_mse', 'max_depth': 4, 'min_samples_split': 0.4518794559933012, 'features': 'log2', 'learning_rate': 0.08549371198423132}. Best is trial 0 with value: 0.822375440008067.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:20:59,982] Trial 1 finished with value: 0.8695530152206121 and parameters: {'n_estimators': 632, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.07154445896835837, 'features': 'log2', 'learning_rate': 0.07037932362623087}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:21:21,930] Trial 2 finished with value: 0.8180757033566141 and parameters: {'n_estimators': 203, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.6391390433159567, 'features': 'log2', 'learning_rate': 0.08766317734145983}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:22:51,751] Trial 3 finished with value: 0.7857966917411402 and parameters: {'n_estimators': 411, 'criterion': 'friedman_mse', 'max_depth': 8, 'min_samples_split': 0.7546106024186817, 'features': 'sqrt', 'learning_rate': 0.025120832517073543}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:24:20,575] Trial 4 finished with value: 0.8563037772355195 and parameters: {'n_estimators': 601, 'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_split': 0.3496025005002025, 'features': 'log2', 'learning_rate': 0.07360154171520802}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:30:08,634] Trial 5 finished with value: 0.8649918906025857 and parameters: {'n_estimators': 670, 'criterion': 'friedman_mse', 'max_depth': 7, 'min_samples_split': 0.3993751479210926, 'features': 'sqrt', 'learning_rate': 0.04981272263020367}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:34:13,130] Trial 6 finished with value: 0.8603075456265278 and parameters: {'n_estimators': 457, 'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_split': 0.3330282378614323, 'features': 'sqrt', 'learning_rate': 0.04143294669245834}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:34:45,253] Trial 7 finished with value: 0.7902171531497785 and parameters: {'n_estimators': 348, 'criterion': 'friedman_mse', 'max_depth': 6, 'min_samples_split': 0.6983721419024336, 'features': 'log2', 'learning_rate': 0.03136595694260868}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:36:14,825] Trial 8 finished with value: 0.8488504899592316 and parameters: {'n_estimators': 999, 'criterion': 'friedman_mse', 'max_depth': 4, 'min_samples_split': 0.7688644564523845, 'features': 'log2', 'learning_rate': 0.06403252111859131}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:37:44,754] Trial 9 finished with value: 0.8203596094357887 and parameters: {'n_estimators': 261, 'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_split': 0.4775941280248428, 'features': 'sqrt', 'learning_rate': 0.038024814284976324}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:43:37,619] Trial 10 finished with value: 0.8688245277849962 and parameters: {'n_estimators': 403, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.21042317842608088, 'features': 'sqrt', 'learning_rate': 0.07689461886296638}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:45:47,684] Trial 11 finished with value: 0.8406573329678783 and parameters: {'n_estimators': 850, 'criterion': 'friedman_mse', 'max_depth': 8, 'min_samples_split': 0.3949403443854401, 'features': 'log2', 'learning_rate': 0.019441902027101325}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:46:11,670] Trial 12 finished with value: 0.7369866099209538 and parameters: {'n_estimators': 329, 'criterion': 'friedman_mse', 'max_depth': 4, 'min_samples_split': 0.8263127569592655, 'features': 'log2', 'learning_rate': 0.012306894589539735}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:49:19,661] Trial 13 finished with value: 0.8677863552690704 and parameters: {'n_estimators': 618, 'criterion': 'friedman_mse', 'max_depth': 7, 'min_samples_split': 0.1346589794874133, 'features': 'log2', 'learning_rate': 0.08637611450792572}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:49:37,410] Trial 14 finished with value: 0.7815088944679803 and parameters: {'n_estimators': 286, 'criterion': 'friedman_mse', 'max_depth': 2, 'min_samples_split': 0.9774486677396941, 'features': 'log2', 'learning_rate': 0.05262894891756701}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:53:21,270] Trial 15 finished with value: 0.7527434819996733 and parameters: {'n_estimators': 988, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.7061978255206008, 'features': 'sqrt', 'learning_rate': 0.004689953960045344}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:54:41,545] Trial 16 finished with value: 0.7112786303069137 and parameters: {'n_estimators': 514, 'criterion': 'friedman_mse', 'max_depth': 11, 'min_samples_split': 0.8934596636341217, 'features': 'sqrt', 'learning_rate': 0.0025179329366544325}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:55:54,739] Trial 17 finished with value: 0.845688013218508 and parameters: {'n_estimators': 179, 'criterion': 'friedman_mse', 'max_depth': 11, 'min_samples_split': 0.4038964798601183, 'features': 'sqrt', 'learning_rate': 0.09416744640831735}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:56:15,139] Trial 18 finished with value: 0.718141814965863 and parameters: {'n_estimators': 105, 'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_split': 0.7322090539119402, 'features': 'sqrt', 'learning_rate': 0.01894872854725975}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:57:05,104] Trial 19 finished with value: 0.7330202760352892 and parameters: {'n_estimators': 831, 'criterion': 'friedman_mse', 'max_depth': 6, 'min_samples_split': 0.9997063115354422, 'features': 'log2', 'learning_rate': 0.0039083227113255944}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 16:58:21,439] Trial 20 finished with value: 0.8403631747288266 and parameters: {'n_estimators': 651, 'criterion': 'friedman_mse', 'max_depth': 4, 'min_samples_split': 0.5510590131576169, 'features': 'log2', 'learning_rate': 0.04275752859938474}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:01:11,044] Trial 21 finished with value: 0.7943579497486292 and parameters: {'n_estimators': 825, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.7492434244142534, 'features': 'sqrt', 'learning_rate': 0.014444657955286584}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:03:05,277] Trial 22 finished with value: 0.8467430132915365 and parameters: {'n_estimators': 398, 'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_split': 0.5931307118835655, 'features': 'sqrt', 'learning_rate': 0.06930072700216441}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:07:48,916] Trial 23 finished with value: 0.846771844514901 and parameters: {'n_estimators': 393, 'criterion': 'friedman_mse', 'max_depth': 6, 'min_samples_split': 0.08494449870839323, 'features': 'sqrt', 'learning_rate': 0.011245401295039597}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:09:21,380] Trial 24 finished with value: 0.8594411001956638 and parameters: {'n_estimators': 121, 'criterion': 'friedman_mse', 'max_depth': 6, 'min_samples_split': 0.07216666144752405, 'features': 'sqrt', 'learning_rate': 0.0674794396576882}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:13:47,943] Trial 25 finished with value: 0.84781317685171 and parameters: {'n_estimators': 891, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.576345144718464, 'features': 'sqrt', 'learning_rate': 0.02897341948444276}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:24:10,482] Trial 26 finished with value: 0.8685725526701682 and parameters: {'n_estimators': 840, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.20028349581022217, 'features': 'sqrt', 'learning_rate': 0.022415345054296616}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:24:35,664] Trial 27 finished with value: 0.7516211176857852 and parameters: {'n_estimators': 160, 'criterion': 'friedman_mse', 'max_depth': 1, 'min_samples_split': 0.8220388381205592, 'features': 'sqrt', 'learning_rate': 0.04376386642148147}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:26:11,838] Trial 28 finished with value: 0.8552136280651093 and parameters: {'n_estimators': 566, 'criterion': 'friedman_mse', 'max_depth': 11, 'min_samples_split': 0.3666269333574457, 'features': 'log2', 'learning_rate': 0.04796851462886772}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:27:21,873] Trial 29 finished with value: 0.8608750510247972 and parameters: {'n_estimators': 310, 'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_split': 0.2672755285746752, 'features': 'log2', 'learning_rate': 0.09783497155646327}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:27:52,128] Trial 30 finished with value: 0.8277951992445516 and parameters: {'n_estimators': 206, 'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_split': 0.4083521439023755, 'features': 'log2', 'learning_rate': 0.06908047849064125}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:34:15,695] Trial 31 finished with value: 0.86564348475687 and parameters: {'n_estimators': 977, 'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_split': 0.28883142372181775, 'features': 'sqrt', 'learning_rate': 0.05900413497248192}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:36:10,660] Trial 32 finished with value: 0.823637655142177 and parameters: {'n_estimators': 895, 'criterion': 'friedman_mse', 'max_depth': 8, 'min_samples_split': 0.46795736206336674, 'features': 'log2', 'learning_rate': 0.01429034991110731}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:38:46,394] Trial 33 finished with value: 0.8463823813470616 and parameters: {'n_estimators': 382, 'criterion': 'friedman_mse', 'max_depth': 4, 'min_samples_split': 0.33662492747637324, 'features': 'sqrt', 'learning_rate': 0.0337194250049433}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:44:08,927] Trial 34 finished with value: 0.8636824138677471 and parameters: {'n_estimators': 891, 'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_split': 0.554357757670264, 'features': 'sqrt', 'learning_rate': 0.05997890223739222}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[I 2024-03-13 17:46:10,410] Trial 35 finished with value: 0.8655529639987066 and parameters: {'n_estimators': 494, 'criterion': 'friedman_mse', 'max_depth': 7, 'min_samples_split': 0.19802236244935462, 'features': 'log2', 'learning_rate': 0.07777683778156683}. Best is trial 1 with value: 0.8695530152206121.\n",
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[W 2024-03-13 17:46:25,159] Trial 36 failed with parameters: {'n_estimators': 764, 'criterion': 'friedman_mse', 'max_depth': 5, 'min_samples_split': 0.6064349172334798, 'features': 'log2', 'learning_rate': 0.009298985712745232} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\AppData\\Local\\Temp\\ipykernel_32376\\261554802.py\", line 22, in objective\n",
      "    score = cross_val_score(model, data_train, y_train, cv=3, scoring='roc_auc')\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 538, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 615, in _fit_stages\n",
      "    raw_predictions = self._fit_stage(\n",
      "                      ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 257, in _fit_stage\n",
      "    tree.fit(X, residual, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 379, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-13 17:46:25,167] Trial 36 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m     27\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m     28\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mRandomSampler()\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     11\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(\n\u001b[0;32m     14\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     15\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m score \u001b[38;5;241m=\u001b[39m cross_val_score(model, data_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    539\u001b[0m     X,\n\u001b[0;32m    540\u001b[0m     y,\n\u001b[0;32m    541\u001b[0m     raw_predictions,\n\u001b[0;32m    542\u001b[0m     sample_weight,\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    544\u001b[0m     X_val,\n\u001b[0;32m    545\u001b[0m     y_val,\n\u001b[0;32m    546\u001b[0m     sample_weight_val,\n\u001b[0;32m    547\u001b[0m     begin_at_stage,\n\u001b[0;32m    548\u001b[0m     monitor,\n\u001b[0;32m    549\u001b[0m )\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    616\u001b[0m     i,\n\u001b[0;32m    617\u001b[0m     X,\n\u001b[0;32m    618\u001b[0m     y,\n\u001b[0;32m    619\u001b[0m     raw_predictions,\n\u001b[0;32m    620\u001b[0m     sample_weight,\n\u001b[0;32m    621\u001b[0m     sample_mask,\n\u001b[0;32m    622\u001b[0m     random_state,\n\u001b[0;32m    623\u001b[0m     X_csc,\n\u001b[0;32m    624\u001b[0m     X_csr,\n\u001b[0;32m    625\u001b[0m )\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 257\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    262\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    270\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    criterion = trial.suggest_categorical(\"criterion\", ['friedman_mse'])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 11)\n",
    "    min_samples_split = trial.suggest_float(\"min_samples_split\", 0.01, 1)\n",
    "    max_features = trial.suggest_categorical(\"features\", ['sqrt', 'log2'])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.1)\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(model, data_train, y_train, cv=3, scoring='roc_auc')\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.RandomSampler()\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe2339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kseni\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3650           55.90m\n",
      "         2           1.3463           56.01m\n",
      "         3           1.3299           55.72m\n",
      "         4           1.3153           55.05m\n",
      "         5           1.3012           55.69m\n",
      "         6           1.2890           55.65m\n",
      "         7           1.2778           56.27m\n",
      "         8           1.2683           55.82m\n",
      "         9           1.2592           55.69m\n",
      "        10           1.2509           55.81m\n",
      "        11           1.2436           55.76m\n",
      "        12           1.2365           55.84m\n",
      "        13           1.2291           55.90m\n",
      "        14           1.2232           55.91m\n",
      "        15           1.2171           55.89m\n",
      "        16           1.2117           55.66m\n",
      "        17           1.2069           55.75m\n",
      "        18           1.2018           55.88m\n",
      "        19           1.1972           55.87m\n",
      "        20           1.1923           56.05m\n",
      "        21           1.1882           56.11m\n",
      "        22           1.1842           56.22m\n",
      "        23           1.1799           56.11m\n",
      "        24           1.1758           56.20m\n",
      "        25           1.1719           56.34m\n",
      "        26           1.1680           56.19m\n",
      "        27           1.1640           56.09m\n",
      "        28           1.1605           56.04m\n",
      "        29           1.1572           56.13m\n",
      "        30           1.1541           56.18m\n",
      "        31           1.1509           56.20m\n",
      "        32           1.1482           56.09m\n",
      "        33           1.1449           56.13m\n",
      "        34           1.1423           56.14m\n",
      "        35           1.1400           56.22m\n",
      "        36           1.1382           56.19m\n",
      "        37           1.1356           56.19m\n",
      "        38           1.1334           56.03m\n",
      "        39           1.1313           56.06m\n",
      "        40           1.1289           56.14m\n",
      "        41           1.1257           56.18m\n",
      "        42           1.1234           56.23m\n",
      "        43           1.1208           56.36m\n",
      "        44           1.1183           56.52m\n",
      "        45           1.1168           56.67m\n",
      "        46           1.1142           56.63m\n",
      "        47           1.1121           56.73m\n",
      "        48           1.1105           56.87m\n",
      "        49           1.1083           57.02m\n",
      "        50           1.1060           57.03m\n",
      "        51           1.1040           57.10m\n",
      "        52           1.1028           57.12m\n",
      "        53           1.1011           57.13m\n",
      "        54           1.0997           57.15m\n",
      "        55           1.0973           57.24m\n",
      "        56           1.0958           57.27m\n",
      "        57           1.0936           57.23m\n",
      "        58           1.0911           57.29m\n",
      "        59           1.0889           57.34m\n",
      "        60           1.0874           57.32m\n",
      "        61           1.0860           57.41m\n",
      "        62           1.0838           57.47m\n",
      "        63           1.0828           57.44m\n",
      "        64           1.0810           57.41m\n",
      "        65           1.0796           57.46m\n",
      "        66           1.0776           57.56m\n",
      "        67           1.0761           57.66m\n",
      "        68           1.0737           57.65m\n",
      "        69           1.0723           57.69m\n",
      "        70           1.0707           57.84m\n",
      "        71           1.0684           57.96m\n",
      "        72           1.0658           57.99m\n",
      "        73           1.0642           58.11m\n",
      "        74           1.0628           58.15m\n",
      "        75           1.0615           58.34m\n",
      "        76           1.0610           58.40m\n",
      "        77           1.0596           58.55m\n",
      "        78           1.0583           58.63m\n",
      "        79           1.0572           58.68m\n",
      "        80           1.0550           58.70m\n",
      "        81           1.0545           58.76m\n",
      "        82           1.0529           58.74m\n",
      "        83           1.0514           58.73m\n",
      "        84           1.0499           58.68m\n",
      "        85           1.0483           58.70m\n",
      "        86           1.0470           58.75m\n",
      "        87           1.0452           58.77m\n",
      "        88           1.0445           58.78m\n",
      "        89           1.0438           58.74m\n",
      "        90           1.0434           58.75m\n",
      "        91           1.0425           58.70m\n",
      "        92           1.0406           58.76m\n",
      "        93           1.0390           58.76m\n",
      "        94           1.0387           58.75m\n",
      "        95           1.0382           58.78m\n",
      "        96           1.0377           58.78m\n",
      "        97           1.0367           58.81m\n",
      "        98           1.0363           58.80m\n",
      "        99           1.0355           58.84m\n",
      "       100           1.0346           58.88m\n",
      "       101           1.0340           58.87m\n",
      "       102           1.0336           58.87m\n",
      "       103           1.0325           58.86m\n",
      "       104           1.0309           58.87m\n",
      "       105           1.0299           58.84m\n",
      "       106           1.0295           58.82m\n",
      "       107           1.0287           58.84m\n",
      "       108           1.0284           58.84m\n",
      "       109           1.0280           58.82m\n",
      "       110           1.0263           58.75m\n",
      "       111           1.0248           58.74m\n",
      "       112           1.0243           58.71m\n",
      "       113           1.0240           58.71m\n",
      "       114           1.0237           58.72m\n",
      "       115           1.0234           58.70m\n",
      "       116           1.0220           58.61m\n",
      "       117           1.0212           58.61m\n",
      "       118           1.0208           58.58m\n",
      "       119           1.0205           58.60m\n",
      "       120           1.0198           58.61m\n",
      "       121           1.0181           58.56m\n",
      "       122           1.0178           58.53m\n",
      "       123           1.0172           58.49m\n",
      "       124           1.0168           58.46m\n",
      "       125           1.0163           58.41m\n",
      "       126           1.0150           58.30m\n",
      "       127           1.0148           58.28m\n",
      "       128           1.0144           58.28m\n",
      "       129           1.0136           58.28m\n",
      "       130           1.0125           58.24m\n",
      "       131           1.0122           58.25m\n",
      "       132           1.0113           58.24m\n",
      "       133           1.0108           58.25m\n",
      "       134           1.0102           58.26m\n",
      "       135           1.0094           58.26m\n",
      "       136           1.0082           58.24m\n",
      "       137           1.0070           58.21m\n",
      "       138           1.0063           58.12m\n",
      "       139           1.0048           58.01m\n",
      "       140           1.0036           57.99m\n",
      "       141           1.0022           57.96m\n",
      "       142           1.0012           57.88m\n",
      "       143           1.0000           57.87m\n",
      "       144           0.9983           57.86m\n",
      "       145           0.9977           57.83m\n",
      "       146           0.9974           57.80m\n",
      "       147           0.9970           57.77m\n",
      "       148           0.9965           57.71m\n",
      "       149           0.9962           57.70m\n",
      "       150           0.9953           57.66m\n",
      "       151           0.9951           57.66m\n",
      "       152           0.9938           57.60m\n",
      "       153           0.9928           57.60m\n",
      "       154           0.9925           57.56m\n",
      "       155           0.9920           57.49m\n",
      "       156           0.9912           57.47m\n",
      "       157           0.9900           57.43m\n",
      "       158           0.9884           57.33m\n",
      "       159           0.9866           57.26m\n",
      "       160           0.9854           57.21m\n",
      "       161           0.9845           57.15m\n",
      "       162           0.9833           57.09m\n",
      "       163           0.9820           57.03m\n",
      "       164           0.9812           56.98m\n",
      "       165           0.9809           56.94m\n",
      "       166           0.9803           56.88m\n",
      "       167           0.9800           56.84m\n",
      "       168           0.9790           56.82m\n",
      "       169           0.9786           56.78m\n",
      "       170           0.9774           56.73m\n",
      "       171           0.9771           56.71m\n",
      "       172           0.9767           56.66m\n",
      "       173           0.9762           56.58m\n",
      "       174           0.9746           56.50m\n",
      "       175           0.9743           56.44m\n",
      "       176           0.9734           56.36m\n",
      "       177           0.9731           56.32m\n",
      "       178           0.9729           56.27m\n",
      "       179           0.9721           56.24m\n",
      "       180           0.9718           56.21m\n",
      "       181           0.9711           56.16m\n",
      "       182           0.9704           56.07m\n",
      "       183           0.9699           56.02m\n",
      "       184           0.9696           55.96m\n",
      "       185           0.9693           55.92m\n",
      "       186           0.9676           55.85m\n",
      "       187           0.9671           55.82m\n",
      "       188           0.9668           55.78m\n",
      "       189           0.9661           55.72m\n",
      "       190           0.9659           55.68m\n",
      "       191           0.9653           55.65m\n",
      "       192           0.9650           55.60m\n",
      "       193           0.9646           55.56m\n",
      "       194           0.9642           55.51m\n",
      "       195           0.9621           55.43m\n",
      "       196           0.9616           55.38m\n",
      "       197           0.9613           55.33m\n",
      "       198           0.9611           55.28m\n",
      "       199           0.9607           55.21m\n",
      "       200           0.9599           55.15m\n",
      "       201           0.9597           55.11m\n",
      "       202           0.9591           55.04m\n",
      "       203           0.9588           54.99m\n",
      "       204           0.9581           54.93m\n",
      "       205           0.9570           54.88m\n",
      "       206           0.9563           54.81m\n",
      "       207           0.9561           54.78m\n",
      "       208           0.9549           54.72m\n",
      "       209           0.9544           54.68m\n",
      "       210           0.9536           54.63m\n",
      "       211           0.9525           54.59m\n",
      "       212           0.9521           54.52m\n",
      "       213           0.9513           54.48m\n",
      "       214           0.9511           54.43m\n",
      "       215           0.9507           54.36m\n",
      "       216           0.9502           54.29m\n",
      "       217           0.9500           54.23m\n",
      "       218           0.9497           54.18m\n",
      "       219           0.9481           54.10m\n",
      "       220           0.9466           54.05m\n",
      "       221           0.9460           53.99m\n",
      "       222           0.9458           53.93m\n",
      "       223           0.9455           53.87m\n",
      "       224           0.9453           53.81m\n",
      "       225           0.9444           53.75m\n",
      "       226           0.9442           53.69m\n",
      "       227           0.9433           53.63m\n",
      "       228           0.9431           53.56m\n",
      "       229           0.9429           53.51m\n",
      "       230           0.9426           53.45m\n",
      "       231           0.9412           53.39m\n",
      "       232           0.9399           53.32m\n",
      "       233           0.9397           53.27m\n",
      "       234           0.9386           53.20m\n",
      "       235           0.9383           53.16m\n",
      "       236           0.9369           53.06m\n",
      "       237           0.9367           53.00m\n",
      "       238           0.9356           52.94m\n",
      "       239           0.9353           52.88m\n",
      "       240           0.9351           52.83m\n",
      "       241           0.9349           52.78m\n",
      "       242           0.9339           52.70m\n",
      "       243           0.9337           52.65m\n",
      "       244           0.9327           52.58m\n",
      "       245           0.9324           52.52m\n",
      "       246           0.9323           52.47m\n",
      "       247           0.9317           52.42m\n",
      "       248           0.9297           52.34m\n",
      "       249           0.9285           52.29m\n",
      "       250           0.9281           52.22m\n",
      "       251           0.9270           52.16m\n",
      "       252           0.9267           52.09m\n",
      "       253           0.9264           52.04m\n",
      "       254           0.9253           51.95m\n",
      "       255           0.9250           51.89m\n",
      "       256           0.9247           51.83m\n",
      "       257           0.9244           51.78m\n",
      "       258           0.9241           51.72m\n",
      "       259           0.9239           51.67m\n",
      "       260           0.9236           51.62m\n",
      "       261           0.9234           51.56m\n",
      "       262           0.9231           51.49m\n",
      "       263           0.9230           51.44m\n",
      "       264           0.9228           51.38m\n",
      "       265           0.9223           51.32m\n",
      "       266           0.9221           51.26m\n",
      "       267           0.9219           51.20m\n",
      "       268           0.9208           51.14m\n",
      "       269           0.9206           51.08m\n",
      "       270           0.9200           51.01m\n",
      "       271           0.9198           50.95m\n",
      "       272           0.9196           50.89m\n",
      "       273           0.9192           50.83m\n",
      "       274           0.9185           50.76m\n",
      "       275           0.9183           50.71m\n",
      "       276           0.9181           50.64m\n",
      "       277           0.9174           50.58m\n",
      "       278           0.9170           50.50m\n",
      "       279           0.9168           50.43m\n",
      "       280           0.9161           50.37m\n",
      "       281           0.9147           50.32m\n",
      "       282           0.9144           50.25m\n",
      "       283           0.9131           50.17m\n",
      "       284           0.9129           50.11m\n",
      "       285           0.9109           50.04m\n",
      "       286           0.9104           49.99m\n",
      "       287           0.9099           49.96m\n",
      "       288           0.9097           49.93m\n",
      "       289           0.9095           49.90m\n",
      "       290           0.9092           49.87m\n",
      "       291           0.9085           49.82m\n",
      "       292           0.9081           49.77m\n",
      "       293           0.9069           49.75m\n",
      "       294           0.9068           49.72m\n",
      "       295           0.9058           49.67m\n",
      "       296           0.9044           49.63m\n",
      "       297           0.9036           49.58m\n",
      "       298           0.9034           49.51m\n",
      "       299           0.9031           49.44m\n",
      "       300           0.9029           49.38m\n",
      "       301           0.9019           49.31m\n",
      "       302           0.9017           49.25m\n",
      "       303           0.9015           49.19m\n",
      "       304           0.9012           49.12m\n",
      "       305           0.9011           49.06m\n",
      "       306           0.8992           48.98m\n",
      "       307           0.8979           48.90m\n",
      "       308           0.8977           48.83m\n",
      "       309           0.8975           48.77m\n",
      "       310           0.8965           48.69m\n",
      "       311           0.8963           48.62m\n",
      "       312           0.8953           48.55m\n",
      "       313           0.8945           48.47m\n",
      "       314           0.8928           48.38m\n",
      "       315           0.8926           48.32m\n",
      "       316           0.8912           48.24m\n",
      "       317           0.8902           48.16m\n",
      "       318           0.8900           48.10m\n",
      "       319           0.8896           48.02m\n",
      "       320           0.8894           47.95m\n",
      "       321           0.8892           47.89m\n",
      "       322           0.8882           47.82m\n",
      "       323           0.8880           47.76m\n",
      "       324           0.8876           47.68m\n",
      "       325           0.8871           47.60m\n",
      "       326           0.8852           47.52m\n",
      "       327           0.8839           47.45m\n",
      "       328           0.8832           47.37m\n",
      "       329           0.8826           47.29m\n",
      "       330           0.8821           47.22m\n",
      "       331           0.8819           47.15m\n",
      "       332           0.8810           47.07m\n",
      "       333           0.8808           47.00m\n",
      "       334           0.8798           46.92m\n",
      "       335           0.8781           46.84m\n",
      "       336           0.8767           46.77m\n",
      "       337           0.8761           46.70m\n",
      "       338           0.8759           46.63m\n",
      "       339           0.8757           46.57m\n",
      "       340           0.8745           46.51m\n",
      "       341           0.8743           46.44m\n",
      "       342           0.8731           46.38m\n",
      "       343           0.8720           46.31m\n",
      "       344           0.8707           46.23m\n",
      "       345           0.8703           46.15m\n",
      "       346           0.8696           46.07m\n",
      "       347           0.8692           46.00m\n",
      "       348           0.8683           45.93m\n",
      "       349           0.8678           45.85m\n",
      "       350           0.8672           45.79m\n",
      "       351           0.8670           45.71m\n",
      "       352           0.8664           45.63m\n",
      "       353           0.8659           45.56m\n",
      "       354           0.8656           45.49m\n",
      "       355           0.8645           45.41m\n",
      "       356           0.8640           45.32m\n",
      "       357           0.8629           45.25m\n",
      "       358           0.8620           45.17m\n",
      "       359           0.8616           45.10m\n",
      "       360           0.8607           45.02m\n",
      "       361           0.8606           44.96m\n",
      "       362           0.8598           44.87m\n",
      "       363           0.8591           44.80m\n",
      "       364           0.8581           44.72m\n",
      "       365           0.8579           44.65m\n",
      "       366           0.8578           44.58m\n",
      "       367           0.8576           44.51m\n",
      "       368           0.8570           44.44m\n",
      "       369           0.8568           44.38m\n",
      "       370           0.8567           44.32m\n",
      "       371           0.8563           44.25m\n",
      "       372           0.8561           44.17m\n",
      "       373           0.8559           44.11m\n",
      "       374           0.8557           44.04m\n",
      "       375           0.8550           43.97m\n",
      "       376           0.8547           43.90m\n",
      "       377           0.8544           43.83m\n",
      "       378           0.8542           43.76m\n",
      "       379           0.8526           43.69m\n",
      "       380           0.8513           43.62m\n",
      "       381           0.8509           43.55m\n",
      "       382           0.8499           43.48m\n",
      "       383           0.8498           43.42m\n",
      "       384           0.8491           43.35m\n",
      "       385           0.8489           43.28m\n",
      "       386           0.8487           43.22m\n",
      "       387           0.8486           43.15m\n",
      "       388           0.8484           43.08m\n",
      "       389           0.8469           43.00m\n",
      "       390           0.8468           42.94m\n",
      "       391           0.8465           42.87m\n",
      "       392           0.8460           42.78m\n",
      "       393           0.8458           42.72m\n",
      "       394           0.8446           42.64m\n",
      "       395           0.8443           42.56m\n",
      "       396           0.8436           42.49m\n",
      "       397           0.8434           42.42m\n",
      "       398           0.8422           42.35m\n",
      "       399           0.8419           42.28m\n",
      "       400           0.8414           42.21m\n",
      "       401           0.8413           42.14m\n",
      "       402           0.8407           42.07m\n",
      "       403           0.8403           42.00m\n",
      "       404           0.8398           41.92m\n",
      "       405           0.8397           41.86m\n",
      "       406           0.8395           41.79m\n",
      "       407           0.8393           41.72m\n",
      "       408           0.8392           41.65m\n",
      "       409           0.8390           41.58m\n",
      "       410           0.8379           41.51m\n",
      "       411           0.8377           41.45m\n",
      "       412           0.8375           41.38m\n",
      "       413           0.8363           41.29m\n",
      "       414           0.8350           41.22m\n",
      "       415           0.8342           41.15m\n",
      "       416           0.8340           41.08m\n",
      "       417           0.8336           41.01m\n",
      "       418           0.8335           40.94m\n",
      "       419           0.8333           40.88m\n",
      "       420           0.8330           40.81m\n",
      "       421           0.8329           40.74m\n",
      "       422           0.8323           40.67m\n",
      "       423           0.8312           40.60m\n",
      "       424           0.8303           40.51m\n",
      "       425           0.8294           40.44m\n",
      "       426           0.8290           40.37m\n",
      "       427           0.8285           40.30m\n",
      "       428           0.8283           40.23m\n",
      "       429           0.8276           40.15m\n",
      "       430           0.8273           40.08m\n",
      "       431           0.8270           40.01m\n",
      "       432           0.8258           39.94m\n",
      "       433           0.8251           39.86m\n",
      "       434           0.8248           39.77m\n",
      "       435           0.8237           39.70m\n",
      "       436           0.8235           39.63m\n",
      "       437           0.8233           39.57m\n",
      "       438           0.8224           39.49m\n",
      "       439           0.8214           39.42m\n",
      "       440           0.8204           39.35m\n",
      "       441           0.8202           39.29m\n",
      "       442           0.8201           39.22m\n",
      "       443           0.8199           39.15m\n",
      "       444           0.8197           39.08m\n",
      "       445           0.8196           39.01m\n",
      "       446           0.8192           38.93m\n",
      "       447           0.8191           38.87m\n",
      "       448           0.8189           38.80m\n",
      "       449           0.8188           38.74m\n",
      "       450           0.8179           38.67m\n",
      "       451           0.8178           38.60m\n",
      "       452           0.8170           38.52m\n",
      "       453           0.8163           38.45m\n",
      "       454           0.8161           38.38m\n",
      "       455           0.8159           38.31m\n",
      "       456           0.8155           38.24m\n",
      "       457           0.8153           38.18m\n",
      "       458           0.8151           38.11m\n",
      "       459           0.8150           38.04m\n",
      "       460           0.8149           37.97m\n",
      "       461           0.8141           37.90m\n",
      "       462           0.8140           37.83m\n",
      "       463           0.8136           37.77m\n",
      "       464           0.8129           37.69m\n",
      "       465           0.8123           37.62m\n",
      "       466           0.8109           37.54m\n",
      "       467           0.8108           37.48m\n",
      "       468           0.8101           37.41m\n",
      "       469           0.8099           37.34m\n",
      "       470           0.8096           37.27m\n",
      "       471           0.8084           37.19m\n",
      "       472           0.8082           37.12m\n",
      "       473           0.8078           37.05m\n",
      "       474           0.8076           36.98m\n",
      "       475           0.8073           36.91m\n",
      "       476           0.8068           36.83m\n",
      "       477           0.8066           36.76m\n",
      "       478           0.8064           36.69m\n",
      "       479           0.8057           36.63m\n",
      "       480           0.8055           36.56m\n",
      "       481           0.8048           36.49m\n",
      "       482           0.8046           36.41m\n",
      "       483           0.8037           36.35m\n",
      "       484           0.8036           36.28m\n",
      "       485           0.8023           36.20m\n",
      "       486           0.8017           36.13m\n",
      "       487           0.8013           36.06m\n",
      "       488           0.8012           35.99m\n",
      "       489           0.8011           35.92m\n",
      "       490           0.8007           35.86m\n",
      "       491           0.8003           35.78m\n",
      "       492           0.8001           35.71m\n",
      "       493           0.7999           35.64m\n",
      "       494           0.7988           35.57m\n",
      "       495           0.7978           35.50m\n",
      "       496           0.7970           35.43m\n",
      "       497           0.7969           35.36m\n",
      "       498           0.7966           35.29m\n",
      "       499           0.7963           35.21m\n",
      "       500           0.7957           35.14m\n",
      "       501           0.7956           35.07m\n",
      "       502           0.7954           35.01m\n",
      "       503           0.7946           34.94m\n",
      "       504           0.7941           34.87m\n",
      "       505           0.7937           34.79m\n",
      "       506           0.7929           34.72m\n",
      "       507           0.7926           34.65m\n",
      "       508           0.7925           34.58m\n",
      "       509           0.7922           34.51m\n",
      "       510           0.7919           34.44m\n",
      "       511           0.7918           34.37m\n",
      "       512           0.7913           34.30m\n",
      "       513           0.7909           34.23m\n",
      "       514           0.7907           34.15m\n",
      "       515           0.7896           34.08m\n",
      "       516           0.7886           34.01m\n",
      "       517           0.7878           33.94m\n",
      "       518           0.7868           33.87m\n",
      "       519           0.7866           33.79m\n",
      "       520           0.7858           33.72m\n",
      "       521           0.7851           33.66m\n",
      "       522           0.7837           33.58m\n",
      "       523           0.7828           33.51m\n",
      "       524           0.7825           33.44m\n",
      "       525           0.7822           33.37m\n",
      "       526           0.7821           33.30m\n",
      "       527           0.7817           33.24m\n",
      "       528           0.7807           33.16m\n",
      "       529           0.7806           33.09m\n",
      "       530           0.7804           33.02m\n",
      "       531           0.7802           32.95m\n",
      "       532           0.7794           32.87m\n",
      "       533           0.7792           32.81m\n",
      "       534           0.7791           32.74m\n",
      "       535           0.7778           32.67m\n",
      "       536           0.7775           32.59m\n",
      "       537           0.7762           32.52m\n",
      "       538           0.7752           32.45m\n",
      "       539           0.7751           32.38m\n",
      "       540           0.7740           32.30m\n",
      "       541           0.7738           32.23m\n",
      "       542           0.7737           32.16m\n",
      "       543           0.7732           32.09m\n",
      "       544           0.7731           32.02m\n",
      "       545           0.7729           31.96m\n",
      "       546           0.7728           31.89m\n",
      "       547           0.7727           31.82m\n",
      "       548           0.7722           31.74m\n",
      "       549           0.7720           31.67m\n",
      "       550           0.7719           31.61m\n",
      "       551           0.7713           31.54m\n",
      "       552           0.7711           31.47m\n",
      "       553           0.7696           31.40m\n",
      "       554           0.7694           31.32m\n",
      "       555           0.7685           31.26m\n",
      "       556           0.7679           31.18m\n",
      "       557           0.7677           31.11m\n",
      "       558           0.7671           31.04m\n",
      "       559           0.7665           30.97m\n",
      "       560           0.7657           30.89m\n",
      "       561           0.7656           30.82m\n",
      "       562           0.7654           30.75m\n",
      "       563           0.7649           30.69m\n",
      "       564           0.7648           30.62m\n",
      "       565           0.7643           30.55m\n",
      "       566           0.7641           30.48m\n",
      "       567           0.7639           30.41m\n",
      "       568           0.7635           30.34m\n",
      "       569           0.7624           30.26m\n",
      "       570           0.7614           30.19m\n",
      "       571           0.7609           30.12m\n",
      "       572           0.7603           30.05m\n",
      "       573           0.7592           29.98m\n",
      "       574           0.7584           29.91m\n",
      "       575           0.7583           29.84m\n",
      "       576           0.7571           29.77m\n",
      "       577           0.7563           29.70m\n",
      "       578           0.7563           29.63m\n",
      "       579           0.7552           29.56m\n",
      "       580           0.7550           29.49m\n",
      "       581           0.7547           29.42m\n",
      "       582           0.7543           29.35m\n",
      "       583           0.7534           29.28m\n",
      "       584           0.7530           29.20m\n",
      "       585           0.7523           29.13m\n",
      "       586           0.7509           29.06m\n",
      "       587           0.7505           28.99m\n",
      "       588           0.7500           28.92m\n",
      "       589           0.7489           28.85m\n",
      "       590           0.7487           28.77m\n",
      "       591           0.7482           28.70m\n",
      "       592           0.7481           28.63m\n",
      "       593           0.7480           28.57m\n",
      "       594           0.7479           28.50m\n",
      "       595           0.7469           28.42m\n",
      "       596           0.7467           28.35m\n",
      "       597           0.7466           28.28m\n",
      "       598           0.7465           28.21m\n",
      "       599           0.7463           28.14m\n",
      "       600           0.7461           28.08m\n",
      "       601           0.7457           28.01m\n",
      "       602           0.7456           27.94m\n",
      "       603           0.7453           27.87m\n",
      "       604           0.7452           27.80m\n",
      "       605           0.7447           27.73m\n",
      "       606           0.7442           27.65m\n",
      "       607           0.7439           27.58m\n",
      "       608           0.7436           27.51m\n",
      "       609           0.7433           27.44m\n",
      "       610           0.7432           27.37m\n",
      "       611           0.7430           27.30m\n",
      "       612           0.7426           27.23m\n",
      "       613           0.7424           27.16m\n",
      "       614           0.7416           27.09m\n",
      "       615           0.7410           27.03m\n",
      "       616           0.7398           26.95m\n",
      "       617           0.7387           26.88m\n",
      "       618           0.7385           26.81m\n",
      "       619           0.7376           26.74m\n",
      "       620           0.7370           26.68m\n",
      "       621           0.7369           26.61m\n",
      "       622           0.7367           26.54m\n",
      "       623           0.7362           26.47m\n",
      "       624           0.7360           26.40m\n",
      "       625           0.7355           26.33m\n",
      "       626           0.7347           26.26m\n",
      "       627           0.7340           26.19m\n",
      "       628           0.7339           26.12m\n",
      "       629           0.7333           26.05m\n",
      "       630           0.7320           25.98m\n",
      "       631           0.7310           25.91m\n",
      "       632           0.7304           25.83m\n",
      "       633           0.7292           25.76m\n",
      "       634           0.7278           25.69m\n",
      "       635           0.7277           25.62m\n",
      "       636           0.7275           25.55m\n",
      "       637           0.7271           25.48m\n",
      "       638           0.7266           25.42m\n",
      "       639           0.7265           25.35m\n",
      "       640           0.7255           25.28m\n",
      "       641           0.7254           25.21m\n",
      "       642           0.7245           25.13m\n",
      "       643           0.7242           25.06m\n",
      "       644           0.7241           24.99m\n",
      "       645           0.7230           24.92m\n",
      "       646           0.7229           24.85m\n",
      "       647           0.7228           24.78m\n",
      "       648           0.7226           24.71m\n",
      "       649           0.7223           24.64m\n",
      "       650           0.7213           24.57m\n",
      "       651           0.7207           24.50m\n",
      "       652           0.7206           24.43m\n",
      "       653           0.7197           24.36m\n",
      "       654           0.7186           24.29m\n",
      "       655           0.7185           24.22m\n",
      "       656           0.7180           24.15m\n",
      "       657           0.7179           24.08m\n",
      "       658           0.7177           24.01m\n",
      "       659           0.7176           23.94m\n",
      "       660           0.7168           23.87m\n",
      "       661           0.7166           23.80m\n",
      "       662           0.7165           23.73m\n",
      "       663           0.7164           23.66m\n",
      "       664           0.7158           23.59m\n",
      "       665           0.7149           23.51m\n",
      "       666           0.7147           23.44m\n",
      "       667           0.7140           23.37m\n",
      "       668           0.7134           23.30m\n",
      "       669           0.7130           23.23m\n",
      "       670           0.7119           23.16m\n",
      "       671           0.7118           23.09m\n",
      "       672           0.7117           23.02m\n",
      "       673           0.7109           22.95m\n",
      "       674           0.7108           22.88m\n",
      "       675           0.7098           22.81m\n",
      "       676           0.7096           22.74m\n",
      "       677           0.7094           22.67m\n",
      "       678           0.7086           22.60m\n",
      "       679           0.7085           22.53m\n",
      "       680           0.7080           22.46m\n",
      "       681           0.7078           22.39m\n",
      "       682           0.7077           22.32m\n",
      "       683           0.7076           22.25m\n",
      "       684           0.7068           22.18m\n",
      "       685           0.7055           22.10m\n",
      "       686           0.7049           22.03m\n",
      "       687           0.7044           21.96m\n",
      "       688           0.7037           21.89m\n",
      "       689           0.7032           21.81m\n",
      "       690           0.7030           21.74m\n",
      "       691           0.7022           21.67m\n",
      "       692           0.7013           21.60m\n",
      "       693           0.7010           21.53m\n",
      "       694           0.7008           21.46m\n",
      "       695           0.7005           21.39m\n",
      "       696           0.6997           21.31m\n",
      "       697           0.6991           21.24m\n",
      "       698           0.6985           21.17m\n",
      "       699           0.6979           21.10m\n",
      "       700           0.6978           21.03m\n",
      "       701           0.6976           20.96m\n",
      "       702           0.6974           20.89m\n",
      "       703           0.6973           20.82m\n",
      "       704           0.6968           20.75m\n",
      "       705           0.6967           20.68m\n",
      "       706           0.6957           20.61m\n",
      "       707           0.6956           20.54m\n",
      "       708           0.6954           20.47m\n",
      "       709           0.6947           20.40m\n",
      "       710           0.6939           20.33m\n",
      "       711           0.6934           20.26m\n",
      "       712           0.6924           20.19m\n",
      "       713           0.6915           20.12m\n",
      "       714           0.6909           20.05m\n",
      "       715           0.6907           19.98m\n",
      "       716           0.6903           19.91m\n",
      "       717           0.6895           19.84m\n",
      "       718           0.6886           19.76m\n",
      "       719           0.6884           19.69m\n",
      "       720           0.6876           19.62m\n",
      "       721           0.6873           19.55m\n",
      "       722           0.6870           19.48m\n",
      "       723           0.6869           19.41m\n",
      "       724           0.6858           19.34m\n",
      "       725           0.6854           19.27m\n",
      "       726           0.6845           19.20m\n",
      "       727           0.6844           19.13m\n",
      "       728           0.6838           19.06m\n",
      "       729           0.6837           18.99m\n",
      "       730           0.6836           18.92m\n",
      "       731           0.6828           18.85m\n",
      "       732           0.6827           18.78m\n",
      "       733           0.6826           18.71m\n",
      "       734           0.6823           18.64m\n",
      "       735           0.6808           18.57m\n",
      "       736           0.6806           18.50m\n",
      "       737           0.6803           18.43m\n",
      "       738           0.6795           18.36m\n",
      "       739           0.6789           18.29m\n",
      "       740           0.6788           18.22m\n",
      "       741           0.6787           18.16m\n",
      "       742           0.6786           18.09m\n",
      "       743           0.6784           18.01m\n",
      "       744           0.6782           17.94m\n",
      "       745           0.6781           17.88m\n",
      "       746           0.6779           17.81m\n",
      "       747           0.6775           17.74m\n",
      "       748           0.6766           17.66m\n",
      "       749           0.6755           17.59m\n",
      "       750           0.6745           17.52m\n",
      "       751           0.6743           17.45m\n",
      "       752           0.6738           17.38m\n",
      "       753           0.6731           17.31m\n",
      "       754           0.6729           17.24m\n",
      "       755           0.6726           17.17m\n",
      "       756           0.6725           17.10m\n",
      "       757           0.6724           17.03m\n",
      "       758           0.6723           16.96m\n",
      "       759           0.6722           16.89m\n",
      "       760           0.6720           16.82m\n",
      "       761           0.6718           16.75m\n",
      "       762           0.6717           16.68m\n",
      "       763           0.6717           16.61m\n",
      "       764           0.6710           16.54m\n",
      "       765           0.6708           16.47m\n",
      "       766           0.6707           16.40m\n",
      "       767           0.6705           16.33m\n",
      "       768           0.6704           16.26m\n",
      "       769           0.6703           16.19m\n",
      "       770           0.6702           16.12m\n",
      "       771           0.6701           16.06m\n",
      "       772           0.6700           15.99m\n",
      "       773           0.6698           15.92m\n",
      "       774           0.6694           15.85m\n",
      "       775           0.6692           15.78m\n",
      "       776           0.6691           15.71m\n",
      "       777           0.6687           15.64m\n",
      "       778           0.6684           15.57m\n",
      "       779           0.6676           15.50m\n",
      "       780           0.6671           15.43m\n",
      "       781           0.6663           15.35m\n",
      "       782           0.6658           15.29m\n",
      "       783           0.6657           15.22m\n",
      "       784           0.6653           15.15m\n",
      "       785           0.6651           15.07m\n",
      "       786           0.6642           15.00m\n",
      "       787           0.6641           14.93m\n",
      "       788           0.6635           14.86m\n",
      "       789           0.6628           14.79m\n",
      "       790           0.6624           14.72m\n",
      "       791           0.6616           14.65m\n",
      "       792           0.6610           14.58m\n",
      "       793           0.6610           14.51m\n",
      "       794           0.6605           14.44m\n",
      "       795           0.6604           14.37m\n",
      "       796           0.6603           14.30m\n",
      "       797           0.6594           14.23m\n",
      "       798           0.6593           14.16m\n",
      "       799           0.6591           14.08m\n",
      "       800           0.6587           14.01m\n",
      "       801           0.6586           13.94m\n",
      "       802           0.6582           13.87m\n",
      "       803           0.6572           13.80m\n",
      "       804           0.6571           13.73m\n",
      "       805           0.6569           13.66m\n",
      "       806           0.6567           13.59m\n",
      "       807           0.6563           13.52m\n",
      "       808           0.6555           13.45m\n",
      "       809           0.6551           13.38m\n",
      "       810           0.6550           13.31m\n",
      "       811           0.6547           13.24m\n",
      "       812           0.6546           13.17m\n",
      "       813           0.6545           13.10m\n",
      "       814           0.6537           13.03m\n",
      "       815           0.6536           12.96m\n",
      "       816           0.6532           12.89m\n",
      "       817           0.6529           12.82m\n",
      "       818           0.6527           12.75m\n",
      "       819           0.6522           12.68m\n",
      "       820           0.6517           12.61m\n",
      "       821           0.6516           12.54m\n",
      "       822           0.6512           12.47m\n",
      "       823           0.6512           12.40m\n",
      "       824           0.6510           12.33m\n",
      "       825           0.6509           12.26m\n",
      "       826           0.6507           12.19m\n",
      "       827           0.6499           12.12m\n",
      "       828           0.6498           12.05m\n",
      "       829           0.6491           11.98m\n",
      "       830           0.6490           11.91m\n",
      "       831           0.6485           11.84m\n",
      "       832           0.6482           11.77m\n",
      "       833           0.6480           11.70m\n",
      "       834           0.6479           11.63m\n",
      "       835           0.6478           11.56m\n",
      "       836           0.6476           11.49m\n",
      "       837           0.6474           11.42m\n",
      "       838           0.6468           11.35m\n",
      "       839           0.6463           11.28m\n",
      "       840           0.6458           11.21m\n",
      "       841           0.6453           11.14m\n",
      "       842           0.6451           11.06m\n",
      "       843           0.6444           10.99m\n",
      "       844           0.6437           10.92m\n",
      "       845           0.6436           10.85m\n",
      "       846           0.6434           10.78m\n",
      "       847           0.6432           10.71m\n",
      "       848           0.6431           10.64m\n",
      "       849           0.6426           10.57m\n",
      "       850           0.6422           10.50m\n",
      "       851           0.6420           10.43m\n",
      "       852           0.6415           10.36m\n",
      "       853           0.6414           10.29m\n",
      "       854           0.6413           10.22m\n",
      "       855           0.6406           10.15m\n",
      "       856           0.6401           10.08m\n",
      "       857           0.6400           10.01m\n",
      "       858           0.6399            9.94m\n",
      "       859           0.6398            9.87m\n",
      "       860           0.6396            9.80m\n",
      "       861           0.6395            9.73m\n",
      "       862           0.6389            9.66m\n",
      "       863           0.6380            9.59m\n",
      "       864           0.6369            9.52m\n",
      "       865           0.6361            9.45m\n",
      "       866           0.6355            9.38m\n",
      "       867           0.6348            9.31m\n",
      "       868           0.6344            9.24m\n",
      "       869           0.6338            9.17m\n",
      "       870           0.6337            9.10m\n",
      "       871           0.6336            9.03m\n",
      "       872           0.6334            8.96m\n",
      "       873           0.6333            8.89m\n",
      "       874           0.6330            8.82m\n",
      "       875           0.6327            8.75m\n",
      "       876           0.6324            8.68m\n",
      "       877           0.6315            8.61m\n",
      "       878           0.6313            8.54m\n",
      "       879           0.6306            8.47m\n",
      "       880           0.6304            8.40m\n",
      "       881           0.6295            8.33m\n",
      "       882           0.6289            8.26m\n",
      "       883           0.6282            8.19m\n",
      "       884           0.6278            8.12m\n",
      "       885           0.6272            8.05m\n",
      "       886           0.6265            7.98m\n",
      "       887           0.6262            7.91m\n",
      "       888           0.6258            7.84m\n",
      "       889           0.6257            7.77m\n",
      "       890           0.6256            7.70m\n",
      "       891           0.6254            7.63m\n",
      "       892           0.6253            7.56m\n",
      "       893           0.6246            7.49m\n",
      "       894           0.6239            7.42m\n",
      "       895           0.6230            7.35m\n",
      "       896           0.6226            7.28m\n",
      "       897           0.6217            7.21m\n",
      "       898           0.6211            7.14m\n",
      "       899           0.6206            7.06m\n",
      "       900           0.6198            6.99m\n",
      "       901           0.6192            6.92m\n",
      "       902           0.6190            6.85m\n",
      "       903           0.6190            6.78m\n",
      "       904           0.6183            6.71m\n",
      "       905           0.6182            6.64m\n",
      "       906           0.6173            6.57m\n",
      "       907           0.6165            6.50m\n",
      "       908           0.6164            6.43m\n",
      "       909           0.6161            6.36m\n",
      "       910           0.6155            6.29m\n",
      "       911           0.6153            6.22m\n",
      "       912           0.6152            6.15m\n",
      "       913           0.6151            6.08m\n",
      "       914           0.6149            6.01m\n",
      "       915           0.6149            5.95m\n",
      "       916           0.6143            5.88m\n",
      "       917           0.6138            5.81m\n",
      "       918           0.6137            5.74m\n",
      "       919           0.6130            5.67m\n",
      "       920           0.6126            5.60m\n",
      "       921           0.6126            5.53m\n",
      "       922           0.6125            5.46m\n",
      "       923           0.6120            5.39m\n",
      "       924           0.6115            5.32m\n",
      "       925           0.6109            5.25m\n",
      "       926           0.6101            5.17m\n",
      "       927           0.6098            5.10m\n",
      "       928           0.6097            5.03m\n",
      "       929           0.6096            4.96m\n",
      "       930           0.6095            4.89m\n",
      "       931           0.6094            4.83m\n",
      "       932           0.6087            4.75m\n",
      "       933           0.6082            4.68m\n",
      "       934           0.6079            4.62m\n",
      "       935           0.6074            4.55m\n",
      "       936           0.6066            4.48m\n",
      "       937           0.6065            4.41m\n",
      "       938           0.6064            4.34m\n",
      "       939           0.6063            4.27m\n",
      "       940           0.6056            4.20m\n",
      "       941           0.6054            4.13m\n",
      "       942           0.6049            4.06m\n",
      "       943           0.6041            3.99m\n",
      "       944           0.6038            3.92m\n",
      "       945           0.6032            3.85m\n",
      "       946           0.6022            3.78m\n",
      "       947           0.6019            3.71m\n",
      "       948           0.6011            3.64m\n",
      "       949           0.6004            3.57m\n",
      "       950           0.5998            3.50m\n",
      "       951           0.5994            3.43m\n",
      "       952           0.5990            3.36m\n",
      "       953           0.5986            3.29m\n",
      "       954           0.5981            3.22m\n",
      "       955           0.5980            3.15m\n",
      "       956           0.5974            3.08m\n",
      "       957           0.5969            3.01m\n",
      "       958           0.5963            2.94m\n",
      "       959           0.5958            2.87m\n",
      "       960           0.5955            2.80m\n",
      "       961           0.5947            2.73m\n",
      "       962           0.5942            2.66m\n",
      "       963           0.5940            2.59m\n",
      "       964           0.5938            2.52m\n",
      "       965           0.5935            2.45m\n",
      "       966           0.5934            2.38m\n",
      "       967           0.5927            2.31m\n",
      "       968           0.5926            2.24m\n",
      "       969           0.5924            2.17m\n",
      "       970           0.5917            2.10m\n",
      "       971           0.5916            2.03m\n",
      "       972           0.5915            1.96m\n",
      "       973           0.5911            1.89m\n",
      "       974           0.5911            1.82m\n",
      "       975           0.5905            1.75m\n",
      "       976           0.5904            1.68m\n",
      "       977           0.5903            1.61m\n",
      "       978           0.5903            1.54m\n",
      "       979           0.5902            1.47m\n",
      "       980           0.5899            1.40m\n",
      "       981           0.5898            1.33m\n",
      "       982           0.5897            1.26m\n",
      "       983           0.5890            1.19m\n",
      "       984           0.5882            1.12m\n",
      "       985           0.5876            1.05m\n",
      "       986           0.5875           58.75s\n",
      "       987           0.5871           54.56s\n",
      "       988           0.5864           50.36s\n",
      "       989           0.5858           46.16s\n",
      "       990           0.5852           41.96s\n",
      "       991           0.5850           37.76s\n",
      "       992           0.5841           33.56s\n",
      "       993           0.5840           29.37s\n",
      "       994           0.5833           25.17s\n",
      "       995           0.5830           20.98s\n",
      "       996           0.5829           16.78s\n",
      "       997           0.5825           12.59s\n",
      "       998           0.5824            8.39s\n",
      "       999           0.5823            4.20s\n",
      "      1000           0.5822            0.00s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.07241534505429661, max_depth=9,\n",
       "                           min_samples_split=0.07102834958102222,\n",
       "                           n_estimators=1000, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.07241534505429661, max_depth=9,\n",
       "                           min_samples_split=0.07102834958102222,\n",
       "                           n_estimators=1000, verbose=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.07241534505429661, max_depth=9,\n",
       "                           min_samples_split=0.07102834958102222,\n",
       "                           n_estimators=1000, verbose=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "best_param = {\n",
    "    'n_estimators': 1000,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'max_depth': 9,\n",
    "    'min_samples_split': 0.071028349581022217,\n",
    "    'learning_rate': 0.072415345054296616\n",
    "}\n",
    "\n",
    "model = GradientBoostingClassifier(**best_param, verbose=3, )\n",
    "model.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872aeffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model ,open(\"model_ver_3.dat\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d83bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-accuracy: 0.6719044034625518  \n",
      "test-recall: [0.66468953 0.67934263] \n",
      "test-f1_score: 0.6709446069642351 \n",
      "test-precision_score: 0.662751677852349 \n",
      "test-roc_auc_score: 0.6720160803144373\n"
     ]
    }
   ],
   "source": [
    "metrics(model.predict(data_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09a7a7a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols_with_missing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_sber.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m useless_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39mcols_with_missing]\n\u001b[0;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mdrop(useless_columns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m test_dataset_cleaned \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mtransform(test_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cols_with_missing' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_parquet('test_sber.parquet', engine='pyarrow')\n",
    "useless_columns = ['id', *cols_with_missing]\n",
    "\n",
    "test_dataset = test_dataset.drop(useless_columns, axis=1)\n",
    "test_dataset_cleaned = imputer.transform(test_dataset)\n",
    "test_dataset_cleaned = pd.DataFrame(test_dataset_cleaned)\n",
    "test_dataset_cleaned.columns = test_dataset.columns\n",
    "test_dataset = scaler.transform(test_dataset_cleaned)\n",
    "\n",
    "pred = model.predict_proba(test_dataset)\n",
    "print(pred)\n",
    "pred_binary = pred.argmax(axis=1)\n",
    "print(pred_binary)\n",
    "pred = pred.max(axis=1)\n",
    "print(pred)\n",
    "pred = [pred[x] if pred_binary[x]==1 else 1-pred[x] for x in range(len(pred_binary))]\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"target_prob\"] = pred\n",
    "submission[\"target_bin\"] = pred_binary\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac117b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import imbPipeline\n",
    "pipeline = imbPipeline([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74008f21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
